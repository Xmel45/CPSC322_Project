{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "935bad99",
   "metadata": {},
   "source": [
    "# Classifying NBA All-Defensive Votes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c6d17e",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction: \n",
    "\n",
    "## Background:\n",
    "In the National Basketball Association (NBA), players can get elected to an All-Defensive team for exemplary defense throughout the season. A set of people were given ballots to fill out with two \"teams\" of players that they thought had the best defense, a first team and a second team. If they nominate a player for the first team, that player gets 2 points and for second team 1 point. The players with the most points get assigned to the All-Defensive teams.\n",
    "## The Task\n",
    "We are building classifiers to look at an NBA player's statistics and team in order to determine if they got put on anyone's ballot (received a vote).\n",
    "## The Data\n",
    "Our dataset [NBA Stats (1947-present)](https://www.kaggle.com/datasets/sumitrodatta/nba-aba-baa-stats?resource=download&select=Team+Stats+Per+Game.csv) came from [Kaggle](https://www.kaggle.com). It was created by Sumitro Datta and contains 22 separate csv files, containing a total of 32 MB of data. It contains player, team, and award data from the past 78 NBA seasons.  \n",
    "We will be using a mixture of 6 of the CSV files; player advanced stats, player basic stats, team basic stats, team advanced stats, opposing team stats, and end of season voting information.\n",
    "\n",
    "## Preview\n",
    "\n",
    "Our best classifiers were Naive Bayes and Random Forest, with 92% and ??% F1 Scores respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ee3c66",
   "metadata": {},
   "source": [
    "\n",
    "# Data Analysis:\n",
    "\n",
    "## Final Dataset\n",
    "\n",
    "As mentioned previously, we joined together 6 seperate tables, but all of those tables needed pruning. Firstly, removed all data from before the 1980 season from each table. The end of season voting data was missing the 2025 voting results, so that was added in manually. We converted the voting results into a binary value, instead of being a total number of votes/ team assignments. We joined the tables with a final count of 35 attributes and 3412 rows.\n",
    "Attributes: \n",
    "* abbreviation (Team Name Abbreviation)\n",
    "* g, gs (Games, Games Started) \n",
    "* pos   (Position)\n",
    "* mp, drb, trb, stl, blk, pf (Minutes Played, Defensive Rebounds, Total Rebounds, Steals, Blocks, Personal Fouls) per game\n",
    "* drb, trb, stl, blk (Defensive Rebound, Total Rebound, Steal, Block) percentage\n",
    "* dws, ws, ws_48 (Defensive Win-Shares, Total Win-Shares, Win-Shares per 48 Minutes)\n",
    "* dbpm, bpm, vorp (Defensive Box Plus-Minus, Box Puls-Minus, Value Over Replacement Player)\n",
    "* voted (Whether or not a player received vote) <- This is what we are classifying\n",
    "* team (Team the player is on and provides team statistics below )\n",
    "* w, l, pw, pl, mov, srs, d_rtg, pace, drb_percent (Wins, Losses, Projected Wins, Projected Losses, Margin of Victory, Simple Rating System, Defensive Rating, Pace, Defensive Rebound Percentage ) \n",
    "* opp_e_fg_percent, opp_tov_percent, opp_ft_fga ( Opponent Effective Field Goal Percentage. Opponent Turnover Percentage, Opponent Free Throw Attempts)\n",
    "\n",
    "\n",
    "Originally we had 24,615 rows with only 1706 positive instances, which caused our classifiers to have high accuracy and low everything else. We decided to randomly select 1706 negative instances from the 22,910 in order to have 50% of our data positve rather than 6.9% split. Below are the before and after balncing results.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0268ce0",
   "metadata": {},
   "source": [
    "<img src=plots/defensive_team_pie_chart.png width = 600> </img> <img src=plots/balanced/defensive_team_pie_chart.png width = 600> </img>\n",
    "<img src=plots/players_per_season_total_vs_voted.png width = 600> </img> <img src=plots/balanced/players_per_season_total_vs_voted.png width = 600> </img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f020766a",
   "metadata": {},
   "source": [
    "# Classification Results: \n",
    "\n",
    "\n",
    "This section should describe the classification approach you developed and its performance. Explain what techniques you used, briefly how you designed and implemented the classifiers, how you evaluated your classifiers’ predictive ability, and how well the classifiers performed. Thoroughly describe how you evaluated performance, the comparison results, and which classifier is “best”.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da067703",
   "metadata": {},
   "source": [
    "# Conclusion: \n",
    "In conclusion, classification of whether or not a player received an All-Defensive team vote based on their stats and team is very possible. The best classifier to use is a Naive Bayes classifier, providing all around high scores (~90%). There is still some inaccuracy, which could possibly be because of the subjectiveness of basketball or due to the natural evolution of offense and defense in the league. Causing some seasons to appear as better defensively when in reality it was just a different dynamic.\n",
    "## Challenges\n",
    "At first we had some trouble with the precision, recall, and F1 Score because our dataset was extremely imbalanced, but once we balanced it out (at the input of our professor) our scores increased greatly. We lost a little bit of accuracy, which is only natural as we no longer had 93% of the dataset as one value. We had a very large dataset, so it was difficult to pick what attributues to use. Additionally, the game of basketball evolves over time and changes from season to season, for instance the average points for a team to score in a game last season was 114, in 2010 it was 104. This can really impact what defensive rating players have purely because it was a different type of game. We also had challenges with some of our old classifiers we developed in class not being efficient enough to handle the large amounts of data we were processing. We had to switch to using numpy arrays in order to get the classifiers to finish in less than 30 minutes.\n",
    "## Improvements\n",
    "We had a couple of ideas for improving our classifier performance, if we split the dataset further into smaller chunks of seasons (5~10 years instead of 45) it could eliminate some of the inaccuracies mentioned above. We could also experiment with more attributes. If we wanted to make our classifier more useful, we could also have it try and predict All-NBA Teams/Votes using different statistics, or try and predict the specific defensive team the players were on.\n",
    "\n",
    "Provide a brief conclusion of your project, including a short summary of the dataset you used (and any of its inherent challenges for classification), the classification approach you developed, your classifiers’ performance, and any ideas you have on ways to improve its performance.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aa245c",
   "metadata": {},
   "source": [
    "# Acknowledgments: \n",
    "Sources : [Kaggle](Kaggle.com), [matplotlib](matplotlib.com), [NBA](NBA.com), [scikit-learn](http://scikit-learn.org/), [Decision Tree Classification in Python](https://www.youtube.com/watch?v=sgQAhG5Q7iY&t=909s), and class materials\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
