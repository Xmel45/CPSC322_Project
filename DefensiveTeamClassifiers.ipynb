{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34d0e0ff",
   "metadata": {},
   "source": [
    "## Defensive Team Classifiers Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "185b9267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful mysklearn package import statements and reloads\n",
    "import importlib\n",
    "\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "# uncomment once you paste your mypytable.py into mysklearn package\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "# uncomment once you paste your myclassifiers.py into mysklearn package\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyKNeighborsClassifier, MyDummyClassifier, MyNaiveBayesClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e36520f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 24616 samples using MyPyTable.\n",
      "Features: ['drb_per_game', 'stl_per_game', 'blk_per_game', 'dws', 'dbpm', 'drb_percent']\n",
      "Sample Raw: [3.8, 0.8, 0.7, 1.7, 0.2, 21.0]\n",
      "Sample Norm: [0.29230769230769227, 0.21621621621621623, 0.11666666666666665, 0.26732673267326734, 0.340958605664488, 0.21]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "table = MyPyTable()\n",
    "table.load_from_file(\"data/Defensive_Team_Data.csv\")\n",
    "\n",
    "\n",
    "# 2. Define the features you want to use\n",
    "feature_names = [\n",
    "    \"drb_per_game\", \"stl_per_game\", \"blk_per_game\", \n",
    "    \"dws\", \"dbpm\", \"drb_percent\"\n",
    "]\n",
    "\n",
    "# This replaces any \"NA\" strings with the column's average, making the column fully numeric.\n",
    "for col in feature_names:\n",
    "    table.replace_missing_values_with_column_average(col)\n",
    "\n",
    "# 4. Extract X (Features)\n",
    "# We find the index of each desired column, then grab those values from table.data\n",
    "col_indices = [table.column_names.index(name) for name in feature_names]\n",
    "\n",
    "X = []\n",
    "for row in table.data:\n",
    "    # Create a new row containing only the selected features\n",
    "    sample = [row[i] for i in col_indices]\n",
    "    X.append(sample)\n",
    "\n",
    "# 5. Extract y (Target)\n",
    "y_col = table.get_column(\"voted\")\n",
    "# Ensure y is integer (0/1) not float (0.0/1.0)\n",
    "y = [int(val) for val in y_col]\n",
    "\n",
    "# 6. Preprocessing\n",
    "X_normalized = myutils.normalize_table(X)\n",
    "#X_discretized = discretize_data(X, n_bins=10)\n",
    "\n",
    "print(f\"Loaded {len(X)} samples using MyPyTable.\")\n",
    "print(f\"Features: {feature_names}\")\n",
    "print(f\"Sample Raw: {X[0]}\")\n",
    "print(f\"Sample Norm: {X_normalized[0]}\")\n",
    "#print(f\"Sample Disc: {X_discretized[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "170bf657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating Dummy Classifier ---\n",
      "Accuracy:  0.931\n",
      "Precision: 0.000\n",
      "Recall:    0.000\n",
      "F1 Score:  0.000\n",
      "Confusion Matrix (0=No, 1=Yes):\n",
      "     Pred 0   Pred 1\n",
      "True 0:  22910      0\n",
      "True 1:  1706      0\n",
      "\n",
      "\n",
      "--- Evaluating kNN (k=5) ---\n",
      "Accuracy:  0.942\n",
      "Precision: 0.615\n",
      "Recall:    0.428\n",
      "F1 Score:  0.504\n",
      "Confusion Matrix (0=No, 1=Yes):\n",
      "     Pred 0   Pred 1\n",
      "True 0:  22453      457\n",
      "True 1:  977      729\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define Classifiers\n",
    "knn_classifier = MyKNeighborsClassifier(n_neighbors=5)\n",
    "dummy_classifier = MyDummyClassifier()\n",
    "# nb_classifier = MyNaiveBayesClassifier()\n",
    "\n",
    "classifiers = [\n",
    "    (\"Dummy Classifier\", dummy_classifier, X_normalized), \n",
    "    (\"kNN (k=5)\", knn_classifier, X_normalized),          \n",
    "    #(\"Naive Bayes\", nb_classifier, X_discretized)         \n",
    "]\n",
    "\n",
    "n_splits = 10\n",
    "folds = myevaluation.stratified_kfold_split(X, y, n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "for name, clf, X_dataset in classifiers:\n",
    "    print(f\"--- Evaluating {name} ---\")\n",
    "    \n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    \n",
    "    for train_idx, test_idx in folds:\n",
    "        X_train = [X_dataset[i] for i in train_idx]\n",
    "        y_train = [y[i] for i in train_idx]\n",
    "        X_test = [X_dataset[i] for i in test_idx]\n",
    "        y_test = [y[i] for i in test_idx]\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        accuracies.append(myevaluation.accuracy_score(y_test, y_pred))\n",
    "        precisions.append(myevaluation.binary_precision_score(y_test, y_pred, pos_label=1))\n",
    "        recalls.append(myevaluation.binary_recall_score(y_test, y_pred, pos_label=1))\n",
    "        f1_scores.append(myevaluation.binary_f1_score(y_test, y_pred, pos_label=1))\n",
    "        \n",
    "        all_y_true.extend(y_test)\n",
    "        all_y_pred.extend(y_pred)\n",
    "\n",
    "    print(f\"Accuracy:  {sum(accuracies)/len(accuracies):.3f}\")\n",
    "    print(f\"Precision: {sum(precisions)/len(precisions):.3f}\")\n",
    "    print(f\"Recall:    {sum(recalls)/len(recalls):.3f}\")\n",
    "    print(f\"F1 Score:  {sum(f1_scores)/len(f1_scores):.3f}\")\n",
    "    \n",
    "    matrix = myevaluation.confusion_matrix(all_y_true, all_y_pred, labels=[0, 1])\n",
    "    print(\"Confusion Matrix (0=No, 1=Yes):\")\n",
    "    print(f\"     Pred 0   Pred 1\")\n",
    "    print(f\"True 0:  {matrix[0][0]}      {matrix[0][1]}\")\n",
    "    print(f\"True 1:  {matrix[1][0]}      {matrix[1][1]}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
