{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "935bad99",
   "metadata": {},
   "source": [
    "# Classifying NBA All-Defensive Votes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c6d17e",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction: \n",
    "\n",
    "## Background:\n",
    "In the National Basketball Association (NBA), players can get elected to an All-Defensive team for exemplary defense throughout the season. A set of people were given ballots to fill out with two \"teams\" of players that they thought had the best defense, a first team and a second team. If they nominate a player for the first team, that player gets 2 points and for second team 1 point. The players with the most points get assigned to the All-Defensive teams.\n",
    "## The Task\n",
    "We are building classifiers to look at an NBA player's statistics and team in order to determine if they got put on anyone's ballot (received a vote).\n",
    "## The Data\n",
    "Our dataset [NBA Stats (1947-present)](https://www.kaggle.com/datasets/sumitrodatta/nba-aba-baa-stats?resource=download&select=Team+Stats+Per+Game.csv) came from [Kaggle](https://www.kaggle.com). It was created by Sumitro Datta and contains 22 separate csv files, containing a total of 32 MB of data. It contains player, team, and award data from the past 78 NBA seasons.  \n",
    "We will be using a mixture of 6 of the CSV files; player advanced stats, player basic stats, team basic stats, team advanced stats, opposing team stats, and end of season voting information.\n",
    "\n",
    "## Preview\n",
    "\n",
    "Our best classifiers were Naive Bayes and Random Forest, with 92% and ??% F1 Scores respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ee3c66",
   "metadata": {},
   "source": [
    "\n",
    "# Data Analysis:\n",
    "\n",
    "## Final Dataset\n",
    "\n",
    "As mentioned previously, we joined together 6 seperate tables, but all of those tables needed pruning. Firstly, removed all data from before the 1980 season from each table. The end of season voting data was missing the 2025 voting results, so that was added in manually. We converted the voting results into a binary value, instead of being a total number of votes/ team assignments. We joined the tables with a final count of 35 attributes and 3412 rows.\n",
    "Attributes: \n",
    "* abbreviation (Team Name Abbreviation)\n",
    "* g, gs (Games, Games Started) \n",
    "* pos   (Position)\n",
    "* mp, drb, trb, stl, blk, pf (Minutes Played, Defensive Rebounds, Total Rebounds, Steals, Blocks, Personal Fouls) per game\n",
    "* drb, trb, stl, blk (Defensive Rebound, Total Rebound, Steal, Block) percentage\n",
    "* dws, ws, ws_48 (Defensive Win-Shares, Total Win-Shares, Win-Shares per 48 Minutes)\n",
    "* dbpm, bpm, vorp (Defensive Box Plus-Minus, Box Puls-Minus, Value Over Replacement Player)\n",
    "* voted (Whether or not a player received vote) <- This is what we are classifying\n",
    "* team (Team the player is on and provides team statistics below )\n",
    "* w, l, pw, pl, mov, srs, d_rtg, pace, drb_percent (Wins, Losses, Projected Wins, Projected Losses, Margin of Victory, Simple Rating System, Defensive Rating, Pace, Defensive Rebound Percentage ) \n",
    "* opp_e_fg_percent, opp_tov_percent, opp_ft_fga ( Opponent Effective Field Goal Percentage. Opponent Turnover Percentage, Opponent Free Throw Attempts)\n",
    "\n",
    "\n",
    "Originally we had 24,615 rows with only 1706 positive instances, which caused our classifiers to have high accuracy and low everything else. We decided to randomly select 1706 negative instances from the 22,910 in order to have 50% of our data positve rather than 6.9% split. Below are the before and after balncing results.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0268ce0",
   "metadata": {},
   "source": [
    "<img src=plots/defensive_team_pie_chart.png width = 600> </img> <img src=plots/balanced/defensive_team_pie_chart.png width = 600> </img>\n",
    "<img src=plots/players_per_season_total_vs_voted.png width = 600> </img> <img src=plots/balanced/players_per_season_total_vs_voted.png width = 600> </img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f020766a",
   "metadata": {},
   "source": [
    "# Classification Results: \n",
    "This section should describe the classification approach you developed and its performance. Explain what techniques you used, briefly how you designed and implemented the classifiers, how you evaluated your classifiers’ predictive ability, and how well the classifiers performed. Thoroughly describe how you evaluated performance, the comparison results, and which classifier is “best”.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da067703",
   "metadata": {},
   "source": [
    "# Conclusion: \n",
    "Provide a brief conclusion of your project, including a short summary of the dataset you used (and any of its inherent challenges for classification), the classification approach you developed, your classifiers’ performance, and any ideas you have on ways to improve its performance.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aa245c",
   "metadata": {},
   "source": [
    "# Acknowledgments: \n",
    "This is where you should cite your sources, including any data, code, or materials that are outside of the scope of CPSC 322 (including previous course projects) that you used. As per the course syllabus, you also need to acknowledge any use of AI.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
